🎙️ Deepfake Audio Detection using CNN and Spectrograms
📌 Overview
This project focuses on detecting deepfake audio using a Convolutional Neural Network (CNN) trained on spectrogram features. As deepfake technologies advance, the ability to identify synthetic voices becomes crucial for maintaining the integrity of audio-based verification systems and preventing misuse.

🧠 Project Objective
  To build a robust classifier that distinguishes between real and fake (deepfake) audio samples using:
  
  Time-frequency features (mel spectrograms)
  
  Deep learning (CNNs)
  
  Evaluation on a labeled dataset split into training, validation, and testing sets

📂 Dataset Structure
  /for-rerecorded/
  ├── training/
  │   ├── real/
  │   └── fake/
  ├── validation/
  │   ├── real/
  │   └── fake/
  └── testing/
      ├── real/
      └── fake/

🔧 Technologies Used
  Python
  Google Colab
  Librosa – Audio loading and feature extraction
  Torchaudio – Advanced preprocessing
  PyTorch – Model building and training
  Matplotlib/Seaborn – Visualization

🧪 Key Steps
    Audio Preprocessing:
      Load .wav files
      Convert waveforms into mel spectrograms for better CNN performance
    
    Model Training:
      Build a CNN architecture
      Train on spectrograms labeled as fake or real
    
    Evaluation:
      Use accuracy, precision, recall, F1-score
      Visualize confusion matrix

    Inference:
      Predict whether a given audio sample is fake or real

