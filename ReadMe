ğŸ™ï¸ Deepfake Audio Detection using CNN and Spectrograms
ğŸ“Œ Overview
This project focuses on detecting deepfake audio using a Convolutional Neural Network (CNN) trained on spectrogram features. As deepfake technologies advance, the ability to identify synthetic voices becomes crucial for maintaining the integrity of audio-based verification systems and preventing misuse.

ğŸ§  Project Objective
  To build a robust classifier that distinguishes between real and fake (deepfake) audio samples using:
  
  Time-frequency features (mel spectrograms)
  
  Deep learning (CNNs)
  
  Evaluation on a labeled dataset split into training, validation, and testing sets

ğŸ“‚ Dataset Structure
  /for-rerecorded/
  â”œâ”€â”€ training/
  â”‚   â”œâ”€â”€ real/
  â”‚   â””â”€â”€ fake/
  â”œâ”€â”€ validation/
  â”‚   â”œâ”€â”€ real/
  â”‚   â””â”€â”€ fake/
  â””â”€â”€ testing/
      â”œâ”€â”€ real/
      â””â”€â”€ fake/

ğŸ”§ Technologies Used
  Python
  Google Colab
  Librosa â€“ Audio loading and feature extraction
  Torchaudio â€“ Advanced preprocessing
  PyTorch â€“ Model building and training
  Matplotlib/Seaborn â€“ Visualization

ğŸ§ª Key Steps
    Audio Preprocessing:
      Load .wav files
      Convert waveforms into mel spectrograms for better CNN performance
    
    Model Training:
      Build a CNN architecture
      Train on spectrograms labeled as fake or real
    
    Evaluation:
      Use accuracy, precision, recall, F1-score
      Visualize confusion matrix

    Inference:
      Predict whether a given audio sample is fake or real

